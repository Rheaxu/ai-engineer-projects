{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4edbe397",
      "metadata": {
        "id": "4edbe397"
      },
      "source": [
        "# Project¬†5: **Build a Multi-Modal Generation Agent**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44fe0ead",
      "metadata": {
        "id": "44fe0ead"
      },
      "source": [
        "Welcome to the final project! In this project, you'll use open-source text-to-image and text-to-video models to generate content. Next, you'll build a **unified multi-modal agent** similar to modern chatbots, where a single agent can support general questions, image generation, and video generation requests.\n",
        "\n",
        "By the end of this project, you'll understand how to integrate multiple model types under one  routing system capable of deciding what modality to use based on the user's intent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3327e3b3",
      "metadata": {
        "id": "3327e3b3"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b842e61",
      "metadata": {
        "id": "8b842e61"
      },
      "source": [
        "* Use **Text-to-Image** models to generate images from a text.\n",
        "* Generate short clips with a **Text-to-Video** model\n",
        "* Build a **Multi-Modal Agent** that answers questions and routes media requests\n",
        "* Build a simple **Gradio** UI and interact with the multi-modal agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c433ac53",
      "metadata": {
        "id": "c433ac53"
      },
      "source": [
        "## Roadmap\n",
        "1. Environment setup\n",
        "2. Text‚Äëto‚ÄëImage\n",
        "3. Text‚Äëto‚ÄëVideo\n",
        "4. Multimodal Agent\n",
        "5. Gradio UI\n",
        "6. Celebrate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd9ad71d",
      "metadata": {
        "id": "fd9ad71d"
      },
      "source": [
        "## 1 - Environment Setup\n",
        "\n",
        "In this project, we'll use open-source Text-to-Image and Text-to-Video models to generate visuals from natural-language prompts. These models are computationally heavy and perform best on GPUs, so we recommend running this notebook in Google Colab or another GPU-enabled environment. We'll load all models from Hugging Face, which requires authentication.\n",
        "\n",
        "Before continuing:\n",
        "1. Open this project in Google Colab. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://colab.research.google.com/github/Rheaxu/ai-engineer-projects/blob/main/Multi_Modal_Gen_Agent/multimodal_agent.ipynb)\n",
        "2. Create a Hugging Face account and generate an access token at huggingface.co/settings/tokens\n",
        "3. Paste your token in the field below to log in.\n",
        "4. In the Colab environment, enable GPU acceleration by selecting Runtime ‚Üí Change runtime type ‚Üí GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yNp2NUkF8p3m",
      "metadata": {
        "id": "yNp2NUkF8p3m"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Set HUGGING_FACE_API_KEY\n",
        "login(token=\"HUGGING_FACE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<fieldset>\n",
        "  <legend><b>Note</b></legend>\n",
        "\n",
        "# AI Development Libraries: Overview\n",
        "\n",
        "The combination of `torch`, `diffusers`, and `transformers` represents the standard \"Hugging Face\" stack for running state-of-the-art Generative AI.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. torch (PyTorch)\n",
        "**The Mathematical Engine.**\n",
        "PyTorch is an open-source machine learning library primarily developed by Meta's AI Research lab.\n",
        "\n",
        "* **Core Function:** It provides the data structures (Tensors) and the computational graphs needed to run deep learning models.\n",
        "* **GPU Acceleration:** It allows code to run on NVIDIA GPUs via CUDA, which is essential for the speed required in AI generation.\n",
        "* **Role:** It is the \"foundation\" that `diffusers` and `transformers` are built upon.\n",
        "\n",
        "## 2. diffusers\n",
        "**The Diffusion Pipeline.**\n",
        "Developed by Hugging Face, this library is the go-to tool for diffusion models (like Stable Diffusion).\n",
        "\n",
        "* **Core Function:** It simplifies the complex math of \"denoising\" (turning random noise into a coherent image).\n",
        "* **Features:** It provides pre-built \"pipelines\" that combine the U-Net, VAE, and Schedulers into a few lines of code.\n",
        "* **Role:** It manages the actual image-creation logic.\n",
        "\n",
        "\n",
        "\n",
        "## 3. transformers\n",
        "**The Language Interpreter.**\n",
        "Another Hugging Face library, this is the industry standard for Natural Language Processing (NLP).\n",
        "\n",
        "* **Core Function:** It provides the \"Text Encoders\" (like CLIP or T5).\n",
        "* **Role:** In a text-to-image workflow, the computer doesn't \"see\" your words. The `transformers` library converts your text prompt into a vector (a string of numbers) that the `diffusers` library can then use as a guide.\n",
        "\n",
        "## 4. gc (Garbage Collector)\n",
        "**The Memory Manager.**\n",
        "A built-in Python module used for memory management.\n",
        "\n",
        "* **Core Function:** It manually triggers the release of memory that is no longer being used by the program.\n",
        "* **Role:** Because AI models are massive, they often fill up your VRAM/RAM. Developers use `gc.collect()` to clear out \"zombie\" data to prevent the dreaded `Out of Memory (OOM)` errors.\n",
        "\n",
        "---\n",
        "\n",
        "### Comparison Summary\n",
        "\n",
        "| Library | Category | Primary Job |\n",
        "| :--- | :--- | :--- |\n",
        "| **torch** | Framework | Low-level math and GPU hardware interface. |\n",
        "| **diffusers** | Generative AI | Image/Video generation via diffusion. |\n",
        "| **transformers** | NLP | Understanding and encoding the text prompt. |\n",
        "| **gc** | Utility | Preventing memory crashes by cleaning up. |\n",
        "\n",
        "---\n",
        "\n",
        "</fieldset>"
      ],
      "metadata": {
        "id": "e_Fc6vVDChgE"
      },
      "id": "e_Fc6vVDChgE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7950f70c",
      "metadata": {
        "id": "7950f70c"
      },
      "outputs": [],
      "source": [
        "import torch, diffusers, transformers, os, random, gc\n",
        "print('torch', torch.__version__, '| CUDA:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028f7c15",
      "metadata": {
        "id": "028f7c15"
      },
      "source": [
        "## 2 - Text-to-Image (T2I)\n",
        "T2I models translate natural-language descriptions into images. They are typically based on diffusion models, which gradually refine random noise into a coherent picture guided by the text prompt. In this section, you'll load and test one such model to generate images directly from text inputs.\n",
        "\n",
        "### 2.1: Load a T2I Model\n",
        "We'll use `Stable Diffusion XL` (SDXL) by `Stability AI`, one of the open-source diffusion models. It provides high-quality, detailed image generation with relatively efficient inference compared to earlier versions.\n",
        "\n",
        "You'll load the model from Hugging Face using the diffusers library, which simplifies running diffusion-based pipelines. To learn more about diffusers, read: https://huggingface.co/docs/diffusers/main/index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<fieldset>\n",
        "  <legend><b>Note</b></legend>\n",
        "\n",
        "# Improve model efficiency\n",
        "Purely loading the `stabilityai/stable-diffusion-xl-base-1.0` model  will cause out of memory. Here are some ways to improve the efficiency.\n",
        "\n",
        "## Method 1: Load the model with FP16 precision\n",
        "By default, neural network models, including deffusion models, are trained and saved with float 32, which means that each weight or each parameter of the neural network is 4 bytes.\n",
        "\n",
        "To make it more efficient, we ask pytorch (through Hugging Face) to set the parameters to flat16 (i.e. 2 bytes).\n",
        "\n",
        "## Method 2: Attention\n",
        "Another way is to make the attention more efficient. As sequence size increases(e.g. in high resolution videos or images), the attention ususally becomes the bottleneck.\n",
        "### Understanding the Attention Bottleneck\n",
        "\n",
        "In Generative AI, **Attention** is the mechanism that allows models to understand relationships between different parts of an input (like words in a prompt or pixels in an image). However, it is also the primary reason high-resolution generation is so computationally expensive.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. What is Attention?\n",
        "Attention allows a model to assign \"importance\" to different parts of an input.\n",
        "\n",
        "* **In Text:** In the sentence \"The **bank** of the river,\" the model uses attention to link \"bank\" to \"river\" so it knows we aren't talking about money.\n",
        "* **In Images:** To draw a person's left eye, the model \"attends\" to the right eye to ensure they are the same color, size, and alignment.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2. The Quadratic Scaling Problem ($O(n^2)$)\n",
        "The \"bottleneck\" exists because of how the math scales. In **Full Self-Attention**, every single token must be compared against every other token.\n",
        "\n",
        "* If you have **$n$** tokens, you must perform **$n \\times n$** operations.\n",
        "* This is known as **Quadratic Complexity**.\n",
        "\n",
        "| Sequence Length ($n$) | Total Operations ($n^2$) | Growth Factor |\n",
        "| :--- | :--- | :--- |\n",
        "| 1,000 | 1,000,000 | Baseline |\n",
        "| 2,000 | 4,000,000 | **4x** more work |\n",
        "| 4,000 | 16,000,000 | **16x** more work |\n",
        "\n",
        "[Image showing a graph of quadratic growth ($n^2$) vs linear growth ($n$) to illustrate computational scaling]\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Why High-Resolution is the Bottleneck\n",
        "When we move from text to high-resolution images or video, the number of tokens ($n$) explodes.\n",
        "\n",
        "#### **The Pixel-to-Token Explosion**\n",
        "In models like Stable Diffusion, the image is broken into \"patches\" (tokens).\n",
        "* **$512 \\times 512$ Image:** Roughly 1,000 patches. This fits on most consumer GPUs.\n",
        "* **$1024 \\times 1024$ Image:** This has **4x** as many pixels. Because of $n^2$ scaling, the attention mechanism requires **16x more memory and computation**.\n",
        "* **Video Generation:** Video adds the dimension of **time**. If a 1-second video has 24 frames, you are essentially trying to run attention on 24 images simultaneously.\n",
        "\n",
        "#### **The Pixel-to-Token Explosion**\n",
        "Attention is what makes the generation \"coherent.\" Without it, the model might draw a hand in one corner of an image and a body in another, but they wouldn't connect properly because the pixels wouldn't \"know\" about each other's existence.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Demonstrating the Bottleneck (Python/PyTorch)\n",
        "This script simulates how the memory usage for the **Attention Matrix** ($QK^T$) grows as the sequence length increases.\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "def measure_attention_memory(seq_len):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    d_model = 64 # Feature dimension\n",
        "    \n",
        "    try:\n",
        "        # Create Query and Key tensors\n",
        "        query = torch.randn(1, 1, seq_len, d_model, device=\"cuda\")\n",
        "        key = torch.randn(1, 1, seq_len, d_model, device=\"cuda\")\n",
        "        \n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        \n",
        "        # This step creates the [seq_len, seq_len] matrix (The Bottleneck)\n",
        "        attention_matrix = torch.matmul(query, key.transpose(-1, -2))\n",
        "        \n",
        "        mem_used = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "        print(f\"Seq Length: {seq_len:>5} | GPU Memory: {mem_used:>8.2f} MB\")\n",
        "        \n",
        "    except RuntimeError:\n",
        "        print(f\"Seq Length: {seq_len} | FAILED: Out of Memory\")\n",
        "\n",
        "lengths = [1024, 2048, 4096, 8192, 16384]\n",
        "for l in lengths:\n",
        "    measure_attention_memory(l)\n",
        "```\n",
        "## 5. How we bypass the bottleneck\n",
        "\n",
        "To generate high-resolution images or long videos without crashing the GPU, researchers use several key architectural strategies:\n",
        "\n",
        "### A. Latent Diffusion\n",
        "Instead of performing attention on raw pixels (e.g., $512 \\times 512 = 262,144$ tokens), the image is compressed by a **VAE (Variational Autoencoder)** into a \"Latent Space.\" For example, a $512 \\times 512$ image becomes a $64 \\times 64$ grid of data. This reduces $n$ from **262,144** to just **4,096**, making the $n^2$ calculation significantly cheaper.\n",
        "\n",
        "### B. FlashAttention\n",
        "This is a memory-efficient way to calculate attention. Standard attention tries to write the entire $n \\times n$ matrix to the GPU's High Bandwidth Memory (HBM). **FlashAttention** breaks the matrix into smaller blocks and calculates them in the GPU's fast cache (SRAM), meaning the giant bottleneck matrix never actually has to exist in its full form in the main VRAM.\n",
        "\n",
        "### C. Tiled VAE & Windowed Attention\n",
        "* **Tiled VAE:** If an image is too large to decode at once, the model breaks it into overlapping \"tiles,\" processes them individually, and stitches them back together.\n",
        "* **Windowed/Local Attention:** Instead of every pixel looking at every other pixel in the entire image, pixels only look at their immediate neighbors (a local window). This changes the math from **Quadratic** ($O(n^2)$) to **Linear** ($O(n)$).\n",
        "\n",
        "### D. Spatio-Temporal Decoupling (For Video)\n",
        "In video models like Sora or Stable Video Diffusion, the model often separates \"Spatial Attention\" (looking at pixels within one frame) from \"Temporal Attention\" (looking at the same pixel across different frames). This prevents the model from having to compare every pixel in Frame 1 to every pixel in Frame 100 simultaneously.\n",
        "\n",
        "</fieldset>"
      ],
      "metadata": {
        "id": "-p0bGixGKKhk"
      },
      "id": "-p0bGixGKKhk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfbedd4",
      "metadata": {
        "id": "8cfbedd4"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "# Define the Stable Diffusion XL model ID from Hugging Face and load the pre-trained model\n",
        "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
        "\n",
        "# Load the model with FP16 precision for efficiency\n",
        "pipe_img = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipe_img.enable_attention_slicing()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06d86e22",
      "metadata": {
        "id": "06d86e22"
      },
      "source": [
        "### 2.2: Generate an image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7822b08",
      "metadata": {
        "id": "b7822b08"
      },
      "outputs": [],
      "source": [
        "# Generate and display an image from a text prompt using the loaded pipeline\n",
        "prompt = \"cinematic photograph of a futuristic neon cityscape at dusk, 35mm lens.\"\n",
        "image = pipe_img(prompt).images[0]\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540d2bb0",
      "metadata": {
        "id": "540d2bb0"
      },
      "source": [
        "### 2.3: Experimenting with \"inference_steps\"\n",
        "\n",
        "The number of inference steps determines how many refinement passes the diffusion model makes. Fewer steps give quicker but less detailed images, while more steps improve clarity and structure at the cost of speed.\n",
        "\n",
        "Try generating images with different step counts and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "139f30dd",
      "metadata": {
        "id": "139f30dd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate an image for different values of num_inference_steps (e.g., 10, 25, 50) and compare sharpness and detail\n",
        "images = []\n",
        "\n",
        "prompt = \"cinematic photograph of a futuristic neon cityscape at dusk, 35mm lens.\"\n",
        "step_list = [5, 15, 30]\n",
        "\n",
        "for steps in step_list:\n",
        "    image = pipe_img(prompt, num_inference_steps=steps, guidance_scale=7.5).images[0]\n",
        "    images.append((steps, image))\n",
        "\n",
        "# Plot results side-by-side\n",
        "plt.figure(figsize=(12, 4))\n",
        "for i, (steps, img) in enumerate(images, 1):\n",
        "    plt.subplot(1, len(images), i)\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"{steps} steps\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa480aeb",
      "metadata": {
        "id": "aa480aeb"
      },
      "source": [
        "### 2.4 (Optional): Visualizing the Diffusion Process\n",
        "Diffusion models start from random noise and iteratively refine it into an image that matches the prompt. If you are curious, visualize all intermediate steps to see how the noise gradually turns into a coherent picture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147767b8",
      "metadata": {
        "id": "147767b8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Run the pipeline with 50 inference steps\n",
        "# Step 2: Capture intermediate latents or images during generation\n",
        "# Step 3: Plot them sequentially to show noise evolving into structure\n",
        "\"\"\"\n",
        "YOUR CODE HERE (~10-12 lines)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e5884c3",
      "metadata": {
        "id": "4e5884c3"
      },
      "source": [
        "### 2.5 (Optional): Experiment with other models.\n",
        "Different text-to-image models vary in speed, style, and visual quality. Try swapping in other open-source diffusion models and compare how their outputs differ in detail, realism, or artistic tone.\n",
        "\n",
        "You can browse available models on Hugging Face here: https://huggingface.co/models?library=diffusers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6933ad43",
      "metadata": {
        "id": "6933ad43"
      },
      "outputs": [],
      "source": [
        "# Step 1: Replace model_id with another text-to-image model from Hugging Face\n",
        "# Step 2: Reload the pipeline and generate a few test images\n",
        "# Step 3: Compare image quality, color balance, and prompt fidelity\n",
        "\"\"\"\n",
        "YOUR CODE HERE\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090367b8",
      "metadata": {
        "id": "090367b8"
      },
      "source": [
        "## 3 - Text-to-Video (T2V)\n",
        "T2V models extend the idea of diffusion from still images to moving sequences. Instead of generating one frame, they create a series of coherent frames that depict motion consistent with the text prompt. These models are computationally heavier and often generate short clips (typically 2-10 seconds).\n",
        "\n",
        "In this section, you'll load an open-source video diffusion model and prepare it for generation.\n",
        "\n",
        "### 3.1: Load a T2V model\n",
        "\n",
        "We'll use the model `damo-vilab/text-to-video-ms-1.7b`, which can produce short video clips from text prompts. This model benefits from a specialized scheduler (DPMSolverMultistepScheduler) that improves stability and speed during sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e261bcd3",
      "metadata": {
        "id": "e261bcd3"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "\n",
        "video_model_id = 'damo-vilab/text-to-video-ms-1.7b'\n",
        "\n",
        "# Load the model with FP16 precision for efficiency\n",
        "pipe_vid = DiffusionPipeline.from_pretrained(video_model_id, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_vid.scheduler = DPMSolverMultistepScheduler.from_config(pipe_vid.scheduler.config)\n",
        "pipe_vid.enable_model_cpu_offload()"
      ],
      "metadata": {
        "id": "EQA65iv3YxBO"
      },
      "id": "EQA65iv3YxBO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "161fa323",
      "metadata": {
        "id": "161fa323"
      },
      "source": [
        "### 3.2: Generate a clip\n",
        "Create a short video clip from a text prompt using a text-to-video model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bXmM7Sa15bn9",
      "metadata": {
        "id": "bXmM7Sa15bn9"
      },
      "outputs": [],
      "source": [
        "# Step 1: Write a text prompt describing the video you want to generate\n",
        "# Step 2: Run the text-to-video pipeline with your chosen prompt\n",
        "prompt = \"astronaut walking on Mars at sunrise\"\n",
        "vid_frames = pipe_vid(prompt, num_inference_steps=25, num_frames=16).frames[0]\n",
        "print(vid_frames.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fCOuEQDmaTU7",
      "metadata": {
        "id": "fCOuEQDmaTU7"
      },
      "source": [
        "### 3.3: Frame inspection\n",
        "Inspect a single frame to sanity-check colors, resolution, and subject positioning before writing a full video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MSvVHiG_7OpC",
      "metadata": {
        "id": "MSvVHiG_7OpC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Select one frame from vid_frames (e.g., index 0)\n",
        "# Step 2: Convert float [0,1] frame to uint8 [0,255]\n",
        "# Step 3: Display as a PIL image\n",
        "Image.fromarray(np.array(vid_frames[0] * 255, dtype=np.uint8))\n",
        "# Alternatively\n",
        "# Image.fromarray((vid_frames[0]*255).astype(np.uint8))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56ea2781",
      "metadata": {
        "id": "56ea2781"
      },
      "source": [
        "### 3.4: Convert frames to MP4\n",
        "Write the generated frames to an MP4 file so you can preview and share the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FfS4oJJc52f9",
      "metadata": {
        "id": "FfS4oJJc52f9"
      },
      "outputs": [],
      "source": [
        "# Step 1: Use diffusers.utils.export_to_video to write vid_frames to an MP4\n",
        "# Step 2: Capture and print the saved video path\n",
        "from diffusers.utils import export_to_video\n",
        "\n",
        "video_path = export_to_video(vid_frames)\n",
        "print(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nwn27N3rcZ2L",
      "metadata": {
        "id": "nwn27N3rcZ2L"
      },
      "source": [
        "### 3.5: Video inspection\n",
        "Play the saved video inside the notebook to check motion and temporal consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1PkAmKxHcUZy",
      "metadata": {
        "id": "1PkAmKxHcUZy"
      },
      "outputs": [],
      "source": [
        "# Display the saved MP4 inline\n",
        "from IPython.display import Video\n",
        "\n",
        "Video(video_path, embed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yVq2AgUKapTh",
      "metadata": {
        "id": "yVq2AgUKapTh"
      },
      "source": [
        "### 3.6 (Optional): Experiment with different configs\n",
        "Increase `num_frames` or decrease `num_inference_steps` to experiment with clip length versus quality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3aac4db",
      "metadata": {
        "id": "d3aac4db"
      },
      "source": [
        "## 4 - Multimodal Generation Agent\n",
        "Now that you have text-to-image, text-to-video, and basic LLM question answering, you will build a single agent that routes user requests to the right capability. The agent will read a prompt, infer intent (chat vs image vs video), and return the appropriate output.\n",
        "\n",
        "### 4.1: Load an LLM for generic queries\n",
        "Use a small LLM as the default chat brain. We will start with `gemma-3-1b-it` and keep the loading logic simple. You can swap to another compact chat model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IZYNuZyiY5EZ",
      "metadata": {
        "id": "IZYNuZyiY5EZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch, textwrap, json, re\n",
        "\n",
        "# Load google/gemma-3-1b-it using Hugging Face\n",
        "\n",
        "model_id = \"google/gemma-3-1b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "gemma_llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vPU7s0zqeeg_",
      "metadata": {
        "id": "vPU7s0zqeeg_"
      },
      "source": [
        "### 4.2: Build a routing mechanism to route requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00767550",
      "metadata": {
        "id": "00767550"
      },
      "outputs": [],
      "source": [
        "def generate_media(prompt: str, mode: str):\n",
        "    # Produce either an image or a short video clip from a text prompt.\n",
        "    if mode == \"image\":\n",
        "      return pipe_img(prompt).images[0]\n",
        "    else:\n",
        "      frames = pipe_vid(prompt, num_inference_steps=25, num_frames=16).frames[0]\n",
        "      return frames\n",
        "\n",
        "def llm_generate(prompt, max_new_tokens=64, temperature=0.7):\n",
        "    # Return a response to the prompt with the loaded gemma\n",
        "    outputs = gemma_llm(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature)\n",
        "    return outputs[0][0][\"generated_text\"][-1][\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HsJf8NeWdKIG",
      "metadata": {
        "id": "HsJf8NeWdKIG"
      },
      "outputs": [],
      "source": [
        "def classify_prompt(prompt: str):\n",
        "    \"\"\"Classify the user prompt into QA, image, or video.\"\"\"\n",
        "\n",
        "    # Step 1: Define a system prompt explaining how to classify requests (qa, image, video)\n",
        "    # Step 2: Format the user message and system message as input to the LLM\n",
        "    # Step 3: Generate a response with llm_generate() and parse it using regex\n",
        "    # Step 4: Extract fields \"type\" and \"expanded_prompt\" from the LLM response\n",
        "    # Step 5: Return a dict with classification results or default to {\"type\": \"qa\"} on failure\n",
        "\n",
        "    system = textwrap.dedent(\"\"\"You are a routing assistant for a multimodal generation system.\n",
        "        Decide whether the USER request is:\n",
        "          ‚Ä¢ a factual or conversational question  ‚Üí  type = \"qa\"\n",
        "          ‚Ä¢ an IMAGE generation request          ‚Üí  type = \"image\"\n",
        "          ‚Ä¢ a VIDEO generation request           ‚Üí  type = \"video\"\n",
        "        If it is for image or video, produce an improved, vivid, detailed `expanded_prompt`.\n",
        "        Respond ONLY in this format: {\"type\": \"...\", \"expanded_prompt\": \"...\"}\n",
        "    \"\"\")\n",
        "\n",
        "    messages = [\n",
        "      [\n",
        "          {\n",
        "              \"role\": \"system\",\n",
        "              \"content\": [{\"type\": \"text\", \"text\": system},]\n",
        "          },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": [{\"type\": \"text\", \"text\": prompt},]\n",
        "          },\n",
        "      ],\n",
        "    ]\n",
        "    response = llm_generate(messages, temperature=0.2)\n",
        "    match = re.search(r'\"type\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"expanded_prompt\"\\s*:\\s*\"([^\"]+)', response)\n",
        "    if match:\n",
        "        try:\n",
        "            result = {\n",
        "              \"type\": match.group(1),\n",
        "              \"expanded_prompt\": match.group(2)\n",
        "            }\n",
        "            return result\n",
        "        except Exception:\n",
        "            pass\n",
        "    # fallback\n",
        "    return {\"type\": \"qa\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<fieldset>\n",
        "  <legend><b>Explain the Regex in the above cell</b></legend>\n",
        "\n",
        "## Explain the regex:\n",
        "```\n",
        "match = re.search(r'\"type\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"expanded_prompt\"\\s*:\\s*\"([^\"]+)', response)\n",
        "```\n",
        "\n",
        "The line of code uses **Regular Expressions (Regex)** to extract values from a string (likely an LLM response). It is specifically looking for a pattern that resembles a JSON object containing the keys `\"type\"` and `\"expanded_prompt\"`.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. The Regex Pattern Explained\n",
        "`r'\"type\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"expanded_prompt\"\\s*:\\s*\"([^\"]+)'`\n",
        "\n",
        "| Segment | Meaning |\n",
        "| :--- | :--- |\n",
        "| `r'...'` | **Raw string:** Tells Python to treat backslashes literally (standard for regex). |\n",
        "| `\"type\"` | Matches the literal text `\"type\"`. |\n",
        "| `\\s*:\\s*` | Matches a colon, allowing for any amount of **whitespace** before or after it. |\n",
        "| `\"([^\"]+)\"` | **Capture Group 1:** Matches the value inside quotes. `[^\"]+` means \"one or more characters that are NOT a double quote.\" |\n",
        "| `\\s*,\\s*` | Matches the comma between the two JSON keys, allowing for whitespace. |\n",
        "| `\"expanded_prompt\"` | Matches the literal text `\"expanded_prompt\"`. |\n",
        "| `\\s*:\\s*` | Matches the colon and surrounding whitespace. |\n",
        "| `\"([^\"]+)` | **Capture Group 2:** Matches the value of the expanded prompt until the next quote. |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Why use this instead of `json.loads()`?\n",
        "In AI engineering, we often use Regex instead of standard JSON parsing for two reasons:\n",
        "\n",
        "1. **Handling \"Chatty\" LLMs:** Models often wrap their JSON in conversational text (e.g., *\"Here is the data: { ... }\"*). `json.loads()` would throw an error, but `re.search()` will ignore the extra text and find the data inside.\n",
        "2. **Resilience:** If the LLM forgets to close the final curly brace `}`, the Regex can still extract the specific fields it found.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. How to use the results in Python\n",
        "After running the search, you access the data using the `.group()` method:\n",
        "\n",
        "```python\n",
        "import re\n",
        "\n",
        "response = 'The model output is: {\"type\": \"image_gen\", \"expanded_prompt\": \"A futuristic city\"}'\n",
        "match = re.search(r'\"type\"\\s*:\\s*\"([^\"]+)\"\\s*,\\s*\"expanded_prompt\"\\s*:\\s*\"([^\"]+)', response)\n",
        "\n",
        "if match:\n",
        "    # group(1) is the first ([^\"]+) - the \"type\"\n",
        "    gen_type = match.group(1)\n",
        "    # group(2) is the second ([^\"]+) - the \"expanded_prompt\"\n",
        "    prompt = match.group(2)\n",
        "    \n",
        "    print(f\"Detected Type: {gen_type}\")\n",
        "    print(f\"Extracted Prompt: {prompt}\")\n",
        "```  \n",
        "### 4. Limitations & Risks\n",
        "\n",
        "* **Escaped Quotes:** This regex will break if your prompt contains escaped quotes (e.g., `\"expanded_prompt\": \"A photo of a \\\"cool\\\" car\"`). It will stop reading at the quote before `cool`.\n",
        "* **Ordering:** This specific regex assumes `\"type\"` comes **before** `\"expanded_prompt\"`. If the LLM swaps the order, the match will fail.\n",
        "* **Nested Objects:** If the JSON structure is deeper than one level, this simple regex will likely fail to capture the correct data.\n",
        "\n",
        "</fieldset>"
      ],
      "metadata": {
        "id": "5ey6xSz6GPfk"
      },
      "id": "5ey6xSz6GPfk"
    },
    {
      "cell_type": "markdown",
      "id": "LiKCWnVaekar",
      "metadata": {
        "id": "LiKCWnVaekar"
      },
      "source": [
        "### 4.3: Build the multimodal agent\n",
        "This agent takes a single user prompt, sends it to the `classify_prompt` to determine what kind of task it is, and then calls the appropriate module:\n",
        "- QA: use the chat LLM to generate an answer\n",
        "- Image: use the text-to-image generator\n",
        "- Video: use the text-to-video generator\n",
        "\n",
        "Start with a simple version first. You can improve it later by adding better prompts, guardrails, and citation handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IW9eRebndgY2",
      "metadata": {
        "id": "IW9eRebndgY2"
      },
      "outputs": [],
      "source": [
        "def multimodal_agent(user_prompt: str):\n",
        "    # Step 1: Classify the request\n",
        "    # Step 2: Route the prompt and generate output\n",
        "\n",
        "    decision = classify_prompt(user_prompt)\n",
        "    kind = decision.get('type', 'qa')\n",
        "    if kind == 'qa':\n",
        "        system = \"You are a helpful assistant.\"\n",
        "        messages = [\n",
        "            [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": [{\"type\": \"text\", \"text\": system},]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [{\"type\": \"text\", \"text\": user_prompt},]\n",
        "                },\n",
        "            ],\n",
        "        ]\n",
        "        return llm_generate(messages)\n",
        "    else:\n",
        "        return generate_media(decision['expanded_prompt'], mode=kind)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NO3yNkY_eqec",
      "metadata": {
        "id": "NO3yNkY_eqec"
      },
      "source": [
        "### 4.4: Test the agent\n",
        "Now let's test your multimodal agent end to end. Each prompt will automatically be routed to the correct capability: text Q&A, image generation, or video generation, and display the corresponding output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff5f1ff",
      "metadata": {
        "id": "8ff5f1ff"
      },
      "outputs": [],
      "source": [
        "from diffusers.utils import export_to_video\n",
        "from IPython.display import display, Video\n",
        "\n",
        "# Step 1: Define a few diverse prompts (QA, image, video)\n",
        "# Step 2: For each prompt, call multimodal_agent and inspect the returned result\n",
        "\n",
        "for p in [\n",
        "    \"What's the capital of Iceland?\",\n",
        "    \"Generate an image of a neon dragon flying over Tokyo at night\",\n",
        "    \"Create a short video of a paper plane folding itself\"\n",
        "]:\n",
        "    result = multimodal_agent(p)\n",
        "    print('\\nPROMPT:', p)\n",
        "    if isinstance(result, str):\n",
        "        print(result)\n",
        "    else:\n",
        "        if hasattr(result, 'save'):\n",
        "            display(result)\n",
        "        else:\n",
        "            vid = export_to_video(result)\n",
        "            print(f\"video path: {vid}\")\n",
        "            display(Video(vid, embed=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zT5d0lnmeGvc",
      "metadata": {
        "id": "zT5d0lnmeGvc"
      },
      "source": [
        "Replace the sample queries with your own and verify that the agent chooses the correct generation path."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a218d0bb",
      "metadata": {
        "id": "a218d0bb"
      },
      "source": [
        "## 5 - Interactive Web UI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e16073ff",
      "metadata": {
        "id": "e16073ff"
      },
      "source": [
        "Launch a simple Gradio web interface so you (or your users) can play with the multimodal agent from the browser.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da883c7",
      "metadata": {
        "id": "4da883c7"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown('# Multimodal Agent')\n",
        "    inp = gr.Textbox(placeholder='Ask or create...')\n",
        "    btn = gr.Button('Submit')\n",
        "    out_text = gr.Markdown()\n",
        "    out_img = gr.Image()\n",
        "    out_vid = gr.Video()\n",
        "\n",
        "    def handle(prompt):\n",
        "        res = multimodal_agent(prompt)\n",
        "        if isinstance(res, str):\n",
        "            return res, None, None\n",
        "        elif hasattr(res, 'save'):\n",
        "            return '', res, None\n",
        "        else:\n",
        "            vid = export_to_video(res)\n",
        "            return '', None, vid\n",
        "\n",
        "    btn.click(handle, inp, [out_text, out_img, out_vid])\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e18ae1",
      "metadata": {
        "id": "37e18ae1"
      },
      "source": [
        "After the UI launches, open the link and generate your own images and videos directly from the browser."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775d6b75",
      "metadata": {
        "id": "775d6b75"
      },
      "source": [
        "## üéâ Congratulations!\n",
        "\n",
        "* You have built a **multi-modal agent** capable of understanding various requests, and routing them to the proper model.\n",
        "* Try experimenting with other T2I and T2V models.\n",
        "* Try making your system more efficient. For example, load a separate lightweight llm for routing, and a more capable llm for QA.\n",
        "\n",
        "\n",
        "üëè **Great job!** Take a moment to celebrate. The techniques you implemented here power many production agents and chatbots."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}